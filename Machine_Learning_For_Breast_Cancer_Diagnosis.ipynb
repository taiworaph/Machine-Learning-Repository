{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING TO AID FACILE BREAST CANCER DIAGNOSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation for Project and Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   One in 8 US women (~12.4%) will develop an invasive form of breast cancer over the course of her lifetime. In 2018, \n",
    "an estimated >200000 new cases of invasive breast cancer is predicted along with over 60000 new-cases of non-invasive \n",
    "breast cancer. [1]\n",
    "Are men immune to to breast cancer? The answer is no. According to breastcancer.org, ~2500 new cases of invasive breast\n",
    "cancer will be diagnosed in men by 2018.This is equivalent to 1 in 1000 male with a potential for having the disease.\n",
    "Besides skin-cancer, breast cancer is the most commonly diagnosed cancer with African-american women more likely at risk \n",
    "and gender along with age contributing to the risk-factor.\n",
    "\n",
    "There are multiple approaches currently available for breast tissue evaluation for cancer cells. All of the approaches involves the use of low-level ionizing x-ray radiation on breast tissue. After a breast exam is done, the expertise of a radiologist is required to convert the mammogram image into an actionable item - Cancer vs no-cancer vs need-further-investigation. Our goal in this project is to demonstrate that a semi, well-trained, Convolutional Neural Network(CNN) can be made to give, cancer along with other malignant tissues, a good detection rate at levels that meet those of well-trained radiologists.[2][3][4] \n",
    "\n",
    "\n",
    "![Breast_Cancer_Incidence_Rate](Breast-Cancer-Incidence-Worldwide.jpg \"Breast Cancer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Web-Scrapping from USF for Downloading Lossless JPEG files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mammogram data files used for this ML project were downloaded from the University of South Florida digital \n",
    "mammography homepage. (http://marathon.csee.usf.edu/Mammography/Database.html) \n",
    "The digital mammography homepage consists of 2600*4 lossless jpeg files and their labels. Each mammogram picture\n",
    "of a subject consists of both RIGHT_CC(Right Craniocaudal), LEFT_CC (Left Craniocaudal), RIGHT_MLO (Right mediolateral oblique)and LEFT_MLO (Left mediolateral oblique)images. \n",
    "\n",
    "Unlike other ML projects were one already has a pre-processed dataset, the mammogram data files used for this project\n",
    "were in their raw format and thus there was over 3 weeks of hard-work that was expended on feature engineering.\n",
    "\n",
    "The thought process for feature engineering was broken down into several pieces as highlighted below:\n",
    "    \n",
    "    Part (I): Label abstraction from the USF web-page with a python-html web-crawling algorithm.\n",
    "    Part (II): Feature(lossless JPEG) abstraction from the USF ftp webpage using python-ftp web-crawling algorithm.\n",
    "    Part (III): Feature(lossless JPEG) transformation from lossless JPEG to png for Machine learning. (A quite     \n",
    "                difficult,messy, and very slow step). Multiple programming syntaxes were used for this \n",
    "                transformation step, including 3rd party programs, command line bash scripting etc.\n",
    "    Part (IV): Final matching of labels and transformed features for Machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the mammogram images were transformed to png files we stumbled on an issue that the image files all had \n",
    "different pixel width and pixel height. For convolution in concvolutional neural networks, one of the input parameters is the pixel width, pixel height and RGB color (i.e a 3 dimensional array); all picture files supplied to the convolutional network must have the similar dimensions. \n",
    "\n",
    "Shown below is a raw image file of the LEFT_CC_Image post conversion from lossless JPEG to png. The file as evident has a very high resolution. However, this resolution cannot be fed into a computer as this would beome a gargantuan matrix/tensor of information that the computer will need to optimize.\n",
    "Instead the approach used was to store the image files in their native .png tranformed format and write a code that does transofrmation of the image files to the right dimension prior to machine learning.\n",
    "\n",
    "\n",
    "\n",
    "![LEFT_CC IMAGE](B_3001_1.LEFT_CC.LJPEG.png \"LEFT_CC_IMAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transformation and Image randomization prior to CNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing all the relevant packages for Image transformation and for Machine Learning on a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Uploading all Packages\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from sklearn.cross_validation import train_test_split\n",
    "print(\"Done Uploading all Packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in all the feature files scrapped from the DDSM web-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9392\n"
     ]
    }
   ],
   "source": [
    "ken1= pd.read_csv('C:\\\\Users\\\\tralabi\\\\Downloads\\\\LCC.csv')\n",
    "ken2= pd.read_csv('C:\\\\Users\\\\tralabi\\\\Downloads\\\\RCC.csv')\n",
    "ken3= pd.read_csv('C:\\\\Users\\\\tralabi\\\\Downloads\\\\LMLO.csv')\n",
    "ken4= pd.read_csv('C:\\\\Users\\\\tralabi\\\\Downloads\\\\RMLO.csv')\n",
    "\n",
    "frames = [ken1, ken2, ken3, ken4]\n",
    "FullFrame = pd.concat(frames)\n",
    "\n",
    "print(len(FullFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading into memory all image native image files (.png) and transforming files to the right shape for CNN ML.\n",
    "#### Reading into memory the corresponding labels(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEFT CRANIOCAUDAL\n",
    "X1L = [] #labels\n",
    "X3L = [] #Images\n",
    "path = \"/Users/taiwoalabi/Downloads/benign_01/case3094\"\n",
    "os.chdir(path)\n",
    "YY2 = [doc for doc in os.listdir() if doc.endswith((\".LEFT_CC.LJPEG.png\"))]\n",
    "#YY2 = [doc for doc in os.listdir() if doc.endswith((\".LEFT_CC.LJPEG.jpg\"))]\n",
    "for ii in range(len(FullFrame)):\n",
    "    for tete in YY:\n",
    "        tete1 = tete.split('.')[0] +'.'+ tete.split('.')[1]+'.'+\"LJPEG\"\n",
    "        tete2 = tete.split('.')[0] +'.'+ tete.split('.')[1]\n",
    "        if tete2 == FullFrame.iat[ii,3]:\n",
    "            X1L.append(FullFrame.iat[ii,1])\n",
    "            ImageName = tete1 + '.png'\n",
    "            #ImageName = tete1 + \".jpg\"\n",
    "            #img = Image.open(ImageName)\n",
    "            WIDTH = 299\n",
    "            HEIGHT = 299\n",
    "            #WIDTH = 350\n",
    "            #HEIGHT = 350\n",
    "            full_size_image = cv2.imread(ImageName)\n",
    "            X3L.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "            #X2.append(array(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the List(image arrays) into an array of array and normalizing the array; Transforming the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4L = np.array(X3L)\n",
    "X4L = X4L.astype('float32')\n",
    "X7L = X4L/255\n",
    "\n",
    "# Transforming the feature labels\n",
    "X5L = []\n",
    "for ii in range(len(X1)):\n",
    "    if X1[ii] == 'Cancer':\n",
    "        X5L.append(1)\n",
    "    elif X1[ii] == 'No_Cancer':\n",
    "        X5L.append(0)\n",
    "\n",
    "X6L = np.array(X5L)\n",
    "y = np_utils.to_categorical(X6L)\n",
    "\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "\n",
    "Print(\"Transformation is done\")\n",
    "\n",
    "# Randomization and splitting of the dataset into a training dataset and a testing dataset\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X7L,y, test_size = 0.15, random_state =0)\n",
    "\n",
    "print(\"The shape of Xtrain[0] is: \", Xtrain[0].shape)\n",
    "print(\"The shape of ytain[0] is: \", ytain[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the Convolutional Neural Network with Keras and Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 concolution\n",
    "#1 Artificial Neural network with 512 nodes\n",
    "#Batch size of 32\n",
    "#Input shape 300x300x3\n",
    "#Loss Function - Binary cross-entropy\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(300, 300, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "# Compile model\n",
    "epochs = 50\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Convolution\n",
    "#2 Deep ANN\n",
    "#Batch size of 32\n",
    "#Input image shape 300x300x3\n",
    "#Loss function - Binary cross-entropy\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(300, 300, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "# Compilation of Model \n",
    "epochs = 10\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xtrain, ytrain, validation_data=(Xtest, ytest), epochs=epochs, batch_size=32)\n",
    "# Evaluation of Model -- Maybe Needs a graphical analysis also\n",
    "scores = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Susan G.Komen https://ww5.komen.org/BreastCancer/Statistics.html\n",
    "\n",
    "[2] Comparison of the Accuracy of Thermography and Mammography in the Detection of Breast Cancer - Aug 2016\n",
    "    (Breast Care(Basel))\n",
    "    \n",
    "[3] A half-second glimpse often lets radiologists identify breast cancer cases even when viewing the mammogram\n",
    "    of the opposite breast - April, 2018 (PNAS) [http://www.pnas.org/content/pnas/113/37/10292.full.pdf]\n",
    "    \n",
    "[4] AI algorithm uses color to better detect breast cancer - July 2016 (AuntMinnie.com) [https://www.auntminnie.com/index.aspx?sec=sup&sub=aic&pag=dis&ItemID=117752]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments and Credits\n",
    "[1] The Digital Database for Screening Mammography - 2001, Medical Physics Publishing\n",
    "\n",
    "[2] Current Status of the Digital Database for Screening Mammography - 1998, Proceedings of the Fourth International\n",
    "    Workshop on Digital Mammography.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
