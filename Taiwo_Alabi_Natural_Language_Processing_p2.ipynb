{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "#Importing the Seaborn graphing toolkit package\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[int(num_test/2):], newsgroups_test.target[int(num_test/2):]\n",
    "dev_data, dev_labels = newsgroups_test.data[:int(num_test/2)], newsgroups_test.target[:int(num_test/2)]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print ('training label shape:', train_labels.shape)\n",
    "print ('test label shape:', test_labels.shape)\n",
    "print ('dev label shape:', dev_labels.shape)\n",
    "print ('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label.\n",
    "\n",
    "[2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training label  0 is  1\n",
      "The training data  0 is  Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "The training label  1 is  3\n",
      "The training data  1 is  \n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "The training label  2 is  2\n",
      "The training data  2 is  \n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "The training label  3 is  0\n",
      "The training data  3 is  I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "The training label  4 is  2\n",
      "The training data  4 is  AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "### STUDENT START ###\n",
    "    for ii in range(num_examples):\n",
    "        print('The training label ', ii, 'is ', train_labels[ii] )\n",
    "        print('The training data ', ii, 'is ', train_data[ii])\n",
    "### STUDENT END ###\n",
    "P1(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
    "\n",
    "[6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the Vocabulary is- 196700.0\n",
      "The average non-zero per sample- 96.70599803343165\n",
      "The fraction of non-zero samples- 0.0035978272269590263\n",
      "The last feature string is: zyxel\n",
      "The first feature string is: 00\n",
      "The shape of the data for confirmation is: (2034, 4)\n",
      "The average number of non-zero samples after specifying 4-vocabulary words are- 0.26843657817109146\n",
      "The size of the bigram and trigram data is- 1391646.0\n",
      "The size of the vocabulary after prunning words that appear in less than 10 documents is- 652.2866273353\n",
      "The fraction of words in the dev data missing from the vocabulary are 0.6676499508357916\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "### STUDENT START ###\n",
    "\n",
    "    YY = (CountVectorizer())\n",
    "    YYY = YY.fit_transform(train_data)\n",
    "    \n",
    "    \n",
    "    TT = np.empty(shape=[len(train_data)])\n",
    "    for ii in range(len(train_data)):\n",
    "        TT[ii] = YYY[ii].nnz\n",
    "\n",
    "    size_vocabulary = sum(TT)\n",
    "    print(\"The size of the Vocabulary is-\", size_vocabulary)\n",
    "    \n",
    "    \n",
    "    average_non_zero_per_example = size_vocabulary/len(train_data)\n",
    "    print(\"The average non-zero per sample-\",average_non_zero_per_example)\n",
    "    \n",
    "    \n",
    "    fraction_non_zero = YYY.nnz/(2034*26879)\n",
    "    print(\"The fraction of non-zero samples-\",fraction_non_zero)\n",
    "    \n",
    "    \n",
    "    print('The last feature string is:', YY.get_feature_names()[len(YY.get_feature_names())-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('The first feature string is:', YY.get_feature_names()[0])\n",
    "    \n",
    "    \n",
    "    YYX = (CountVectorizer(train_data, vocabulary = [\"atheism\", \"graphics\", \"space\", \"religion\"]))\n",
    "    YYYX = YYX.fit_transform(train_data)\n",
    "    \n",
    "    \n",
    "    TTT = np.empty(shape=[len(train_data)])\n",
    "    for ii in range(len(train_data)):\n",
    "        TTT[ii] = YYYX[ii].nnz\n",
    "    \n",
    "    size_vocabulary2 = sum(TTT)\n",
    "    average_non_zero_per_example2 = size_vocabulary2/len(train_data)\n",
    "    print(\"The shape of the data for confirmation is:\", YYYX.shape)\n",
    "    print(\"The average number of non-zero samples after specifying 4-vocabulary words are-\", average_non_zero_per_example2)\n",
    "\n",
    "    \n",
    "    \n",
    "    ##Extracting bigram and trigram character features##\n",
    "    bigram_trigram = CountVectorizer(analyzer = 'char', ngram_range= (2,3))\n",
    "    YYYTT = bigram_trigram.fit_transform(train_data)\n",
    "    \n",
    "    ##Size of vocabulary with bigram and trigram data##\n",
    "    TTR = np.empty(shape=[len(train_data)])\n",
    "    for ii in range(len(train_data)):\n",
    "        TTR[ii] = YYYTT[ii].nnz\n",
    "    \n",
    "    size_vocabulary3 = sum(TTR)\n",
    "    print(\"The size of the bigram and trigram data is-\", size_vocabulary3)\n",
    "    \n",
    "    \n",
    "    ##Using the min_df to prune words appearing in less than 10 documents to improve speed##\n",
    "\n",
    "    bigram_trigram_mindf = CountVectorizer(analyzer = 'char', ngram_range= (2,3), min_df=10)\n",
    "    YYYTTX = bigram_trigram_mindf.fit_transform(train_data)\n",
    "\n",
    "    TTTR = np.empty(shape=[len(train_data)])\n",
    "    for ii in range(len(train_data)):\n",
    "        TTTR[ii] = YYYTTX[ii].nnz\n",
    "    \n",
    "    size_vocabulary3 = sum(TTTR)\n",
    "    average_non_zero_per_example3 = size_vocabulary3/len(train_data)\n",
    "    print(\"The size of the vocabulary after prunning words that appear in less than 10 documents is-\", average_non_zero_per_example3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Train-dataset#\n",
    "    YY = (CountVectorizer())\n",
    "    YYY = YY.fit_transform(train_data)\n",
    "\n",
    "    #dev-dataset#\n",
    "    UU = (CountVectorizer())\n",
    "    UUU = UU.fit_transform(dev_data)\n",
    "    \n",
    "    train_shape= list(YYY.shape)\n",
    "    dev_shape = list(UUU.shape)\n",
    "    \n",
    "    Fraction_of_Missing_Vocabulary = (train_shape[0] - dev_shape[0])/train_shape[0]\n",
    "    print(\"The fraction of words in the dev data missing from the vocabulary are\", Fraction_of_Missing_Vocabulary)\n",
    "    \n",
    "### STUDENT END ###\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "The Nearest neighbour algorithm does not work well for Natural Language processing because the idea of using contextual words to define a sentence classification is not easily comprehensible with a KNN algorithm\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "Naive Bayes uses a probabilistic approach that approximates the sentence based on the individual contribution of each words within the sentence -- This is more ideal for Natural Language Processing than a logistic regression using unigram weights of each word in a sentence.\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C.\n",
    "\n",
    "The sum of weights increases (doubles) for every 0.01 increase in C.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a KNN Classifier for K_Nearest_Neighbour = 1 and the f1_score is\n",
      "0.41506646971935\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 3 and the f1_score is\n",
      "0.37518463810930575\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 5 and the f1_score is\n",
      "0.38847858197932056\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 7 and the f1_score is\n",
      "0.3988183161004431\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 9 and the f1_score is\n",
      "0.4047267355982275\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 11 and the f1_score is\n",
      "0.41063515509601184\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 13 and the f1_score is\n",
      "0.41654357459379615\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 15 and the f1_score is\n",
      "0.4254062038404727\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 17 and the f1_score is\n",
      "0.4209748892171344\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 19 and the f1_score is\n",
      "0.41949778434268836\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 21 and the f1_score is\n",
      "0.41654357459379615\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 23 and the f1_score is\n",
      "0.40029542097488924\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 25 and the f1_score is\n",
      "0.41654357459379615\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 27 and the f1_score is\n",
      "0.4327917282127031\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 30 and the f1_score is\n",
      "0.42836041358936483\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 40 and the f1_score is\n",
      "0.413589364844904\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 50 and the f1_score is\n",
      "0.44165435745937964\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 100 and the f1_score is\n",
      "0.4667651403249631\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 1000 and the f1_score is\n",
      "0.3515509601181684\n",
      "This is a KNN Classifier for K_Nearest_Neighbour = 2000 and the f1_score is\n",
      "0.2555391432791728\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  0.0 is  0.7607090103397341\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  0.0001 is  0.7740029542097489\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  0.001 is  0.7784342688330872\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  0.01 is  0.7813884785819794\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  0.1 is  0.7769571639586411\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  0.5 is  0.7695716395864106\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  1.0 is  0.7666174298375185\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  2.0 is  0.7592319054652881\n",
      "The accuracy of the training-size with Multinomial Bernoulli NB and  alpha =  10.0 is  0.7104874446085672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiwoalabi/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.01 is  0.6986706056129985\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.02 is  0.7134416543574594\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.03 is  0.725258493353028\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.04 is  0.7370753323485968\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.05 is  0.7370753323485968\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.1 is  0.7385524372230429\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.2 is  0.740029542097489\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.3 is  0.7341211225997046\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.4 is  0.7311669128508124\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.5 is  0.7282127031019202\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.6 is  0.7282127031019202\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.7 is  0.7282127031019202\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.8 is  0.725258493353028\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  0.9 is  0.7296898079763663\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.0 is  0.7296898079763663\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.1 is  0.7311669128508124\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.2 is  0.7193500738552437\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.3 is  0.723781388478582\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.4 is  0.7193500738552437\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.5 is  0.7267355982274741\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.6 is  0.7223042836041359\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.7 is  0.7178729689807977\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.8 is  0.725258493353028\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  1.9 is  0.7163958641063516\n",
      "The accuracy of the training-size with Logistic_Regression and  C-Regularization Strength =  2.0 is  0.7208271787296898\n",
      "    Sum_of_Squared_Weight  C-Strength\n",
      "0                1.529798        0.01\n",
      "1                2.030293        0.01\n",
      "2                1.866147        0.01\n",
      "3                1.369492        0.01\n",
      "4                3.181892        0.02\n",
      "5                3.860958        0.02\n",
      "6                3.735014        0.02\n",
      "7                2.848191        0.02\n",
      "8                4.759309        0.03\n",
      "9                5.502943        0.03\n",
      "10               5.475081        0.03\n",
      "11               4.272865        0.03\n",
      "12               6.256032        0.04\n",
      "13               7.014067        0.04\n",
      "14               7.112767        0.04\n",
      "15               5.635974        0.04\n",
      "16               7.698193        0.05\n",
      "17               8.437172        0.05\n",
      "18               8.675235        0.05\n",
      "19               6.955241        0.05\n",
      "20              14.123989        0.10\n",
      "21              14.488011        0.10\n",
      "22              15.512702        0.10\n",
      "23              12.888025        0.10\n",
      "24              24.316747        0.20\n",
      "25              23.596308        0.20\n",
      "26              26.108596        0.20\n",
      "27              22.384389        0.20\n",
      "28              33.150754        0.30\n",
      "29              31.194403        0.30\n",
      "..                    ...         ...\n",
      "70              84.507855        1.30\n",
      "71              78.890128        1.30\n",
      "72              87.553009        1.40\n",
      "73              75.809217        1.40\n",
      "74              87.300398        1.40\n",
      "75              81.009825        1.40\n",
      "76              93.000390        1.50\n",
      "77              79.398181        1.50\n",
      "78              92.080754        1.50\n",
      "79              86.043901        1.50\n",
      "80              93.679079        1.60\n",
      "81              79.929680        1.60\n",
      "82              92.606080        1.60\n",
      "83              86.740109        1.60\n",
      "84              96.633049        1.70\n",
      "85              82.818435        1.70\n",
      "86              95.530062        1.70\n",
      "87              89.667587        1.70\n",
      "88             101.802681        1.80\n",
      "89              86.479033        1.80\n",
      "90             100.086750        1.80\n",
      "91              94.462072        1.80\n",
      "92             104.750160        1.90\n",
      "93              89.282463        1.90\n",
      "94             103.040333        1.90\n",
      "95              97.003493        1.90\n",
      "96             106.777587        2.00\n",
      "97              90.634585        2.00\n",
      "98             105.168125        2.00\n",
      "99              99.125006        2.00\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "count_vect= CountVectorizer()\n",
    "YYY = count_vect.fit_transform(train_data)\n",
    "YYZ = count_vect.transform(test_data)\n",
    "k_values = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,30,40,50,100,1000,2000]\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "Regular_Strength = [0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0]\n",
    "\n",
    "def Transformation(k_values):\n",
    "    for ii in k_values:\n",
    "        KNNClass = KNeighborsClassifier(ii)\n",
    "        KNNClass.fit(YYY, train_labels)\n",
    "        pred = KNNClass.predict(YYZ)\n",
    "        print(\"This is a KNN Classifier for K_Nearest_Neighbour =\", ii, \"and the f1_score is\")\n",
    "        print(metrics.f1_score(test_labels, pred, average = 'micro'))\n",
    "\n",
    "\n",
    "\n",
    "def MultiBernoulli(alphas):\n",
    "    \n",
    "    for ii in alphas['alpha']:\n",
    "        MBern = MultinomialNB(alpha = ii)\n",
    "        MBern.fit(YYY, train_labels)\n",
    "        predict = MBern.predict(YYZ)\n",
    "\n",
    "        correct, total = 0, 0\n",
    "        for pred, label in zip(predict, test_labels):\n",
    "            if pred == label: \n",
    "                correct += 1\n",
    "            total += 1\n",
    "        print (\"The accuracy of the training-size with Multinomial Bernoulli NB and \", \"alpha = \", ii,\"is \",  correct/total)\n",
    "\n",
    "        \n",
    "def Logistic_Regress(Regular_Strength):\n",
    "    ##Running a Logistic Regression Model###\n",
    "    TT = []\n",
    "    RR= []\n",
    "    for ii in Regular_Strength:\n",
    "        \n",
    "        LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = ii, solver='lbfgs')\n",
    "        YY = LG.fit(YYY, train_labels)\n",
    "        predict = LG.predict(YYZ)\n",
    "        \n",
    "        correct, total = 0, 0\n",
    "        for pred, label in zip(predict, test_labels):\n",
    "            if pred == label: \n",
    "                correct += 1\n",
    "            total += 1\n",
    "        print (\"The accuracy of the training-size with Logistic_Regression and \", \"C-Regularization Strength = \", ii,\"is \",  correct/total)\n",
    "        XXX= YY.coef_\n",
    "        \n",
    "        \n",
    "        for iii in range(XXX.shape[0]):\n",
    "            UU= sum(XXX[iii]**2)\n",
    "            TT.append(UU)\n",
    "            RR.append(ii)\n",
    "    True1= np.vstack((TT,RR))\n",
    "    True1= True1.transpose()\n",
    "    Column_Names= ['Sum_of_Squared_Weight', 'C-Strength']\n",
    "    True2 = pd.DataFrame(True1,columns= Column_Names)\n",
    "    print(True2)\n",
    "    \n",
    "    \n",
    "def P3():\n",
    "\n",
    "    ### STUDENT START ###\n",
    "\n",
    "    Transformation(k_values)\n",
    "    nb = MultiBernoulli(alphas)  \n",
    "    nb2 = Logistic_Regress(Regular_Strength)\n",
    "\n",
    "\n",
    "    ### STUDENT END ###\n",
    "P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:Because the KNN algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?\n",
    "\n",
    "[5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the 5 features with the largest weights for unigram features\n",
      "   Sum_of_Squared_Weight    Features\n",
      "0     0.1724792845066609       bobby\n",
      "1    0.17978389228745692       space\n",
      "2     0.2079194266085295    atheists\n",
      "3     0.2524084335233957    religion\n",
      "4    0.21493615455930037     atheism\n",
      "5    0.22083400653828122       files\n",
      "6    0.27148078593525476          3d\n",
      "7     0.3746335849947481        file\n",
      "8     0.4233994610594327       image\n",
      "9     0.8826271033386048    graphics\n",
      "10   0.18641107546624275      launch\n",
      "11   0.20813151296222251    graphics\n",
      "12   0.28728367240230257        nasa\n",
      "13   0.31611531523988273       orbit\n",
      "14    1.4067575789983957       space\n",
      "15   0.17412556658934383       blood\n",
      "16   0.17821992892763888       space\n",
      "17   0.25694865914332105   christian\n",
      "18   0.26139009297545523         god\n",
      "19    0.2912939299739354  christians\n",
      "\n",
      "These are the 5 features with the largest weights for bi-gram and tri-gram features\n",
      "   Sum_of_Squared_Weight      Features\n",
      "0    0.07559537648050002          1700\n",
      "1    0.07761534196969229           acc\n",
      "2    0.09281273840909697           ack\n",
      "3    0.09591206714679694      _agapate\n",
      "4    0.25277517849211695           945\n",
      "5    0.12498965430824188  _antiquities\n",
      "6    0.13140098464076713           abu\n",
      "7     0.1337829941750847         94086\n",
      "8    0.20167918159927337       100megs\n",
      "9     0.1366567232889953    adulteries\n",
      "10    0.1495964316043911          adrg\n",
      "11   0.17685704299443297     actuality\n",
      "12   0.21963685618235776     afarensis\n",
      "13   0.21520965100404735          500m\n",
      "14   0.15050639183066275          8306\n",
      "15   0.08943466584683876         acorn\n",
      "16   0.09321895157379671  accommodated\n",
      "17   0.09739159637978317          adam\n",
      "18    0.1019441789861554           805\n",
      "19   0.13925919135259607        _free_\n"
     ]
    }
   ],
   "source": [
    "count_vect= CountVectorizer()\n",
    "YYY = count_vect.fit_transform(train_data)\n",
    "YYZ = count_vect.transform(test_data)\n",
    "WeightsUni = []\n",
    "IndexUni = []\n",
    "WeightsBi = []\n",
    "IndexBi = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def P4():\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "\n",
    "    LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = 0.2, solver='lbfgs')\n",
    "    YY = LG.fit(YYY, train_labels)\n",
    "    XXX = YY.coef_\n",
    "    print(\"These are the 5 features with the largest weights for unigram features\")\n",
    "    for iii in range(XXX.shape[0]):\n",
    "        XXXX = XXX**2\n",
    "        indices = np.argpartition(XXXX[iii], -5)[-5:]\n",
    "        #print(XXXX[iii][indices])\n",
    "        IndexUni.extend(list(XXXX[iii][indices]))\n",
    "        for ii in list(indices):\n",
    "            #print(count_vect.get_feature_names()[ii], ' ')\n",
    "            WeightsUni.append(count_vect.get_feature_names()[ii])\n",
    "    \n",
    "    True1= np.vstack((IndexUni,WeightsUni))\n",
    "    True1= True1.transpose()\n",
    "    Column_Names= ['Sum_of_Squared_Weight', 'Features']\n",
    "    True2 = pd.DataFrame(True1,columns= Column_Names)\n",
    "    print(True2)\n",
    "    \n",
    "    bigram_trigram = CountVectorizer(analyzer = 'char', ngram_range= (2,2))\n",
    "    YYYTT = bigram_trigram.fit_transform(train_data)\n",
    "\n",
    "\n",
    "    LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = 0.2, solver='lbfgs')\n",
    "    YY = LG.fit(YYYTT, train_labels)\n",
    "    XXX = YY.coef_\n",
    "    print(\"\")\n",
    "    print(\"These are the 5 features with the largest weights for bi-gram and tri-gram features\")\n",
    "    for iii in range(XXX.shape[0]):\n",
    "        XXXX= XXX**2\n",
    "        indices = np.argpartition(XXXX[iii], -5)[-5:]\n",
    "        #print(XXXX[iii][indices])\n",
    "        IndexBi.extend(list(XXXX[iii][indices]))\n",
    "        for ii in list(indices):\n",
    "            #print(count_vect.get_feature_names()[ii], ' ')\n",
    "            WeightsBi.append(count_vect.get_feature_names()[ii])\n",
    "            \n",
    "    True3= np.vstack((IndexBi,WeightsBi))\n",
    "    True3= True3.transpose()\n",
    "    Column_Names1= ['Sum_of_Squared_Weight', 'Features']\n",
    "    True4 = pd.DataFrame(True3,columns= Column_Names1)\n",
    "    print(True4)\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Interesting words that appear with bigram feature enablement are words with \"_\" eg _free_, _agapate etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the length of the vocabulary without pre-processing\n",
      "26879\n",
      "The accuracy of the training-dev data with Logistic_Regression and C-Regularization Strength = 0.2 is  0.6997041420118343\n",
      "This is the length of the vocabulary post pre-processing\n",
      "12608\n",
      "The accuracy of the training-dev data with Logistic_Regression and C-Regularization Strength = 0.2 with custom pre-processing is 0.6405325443786982\n"
     ]
    }
   ],
   "source": [
    "def better_preprocessor(s):\n",
    "### STUDENT START ###\n",
    "\n",
    "        ###Creating a word tokenizer that accepts a group of words###\n",
    "    \n",
    "    #This can be considered my tokenizer function#\n",
    "    words1= s.split(' ')\n",
    "    words3= []\n",
    "    for words2 in words1:\n",
    "        \n",
    "        #The digits are selected and replaced with 999#\n",
    "        b = re.compile(r'^\\d+$')\n",
    "        c = re.compile(r'^\\w+$')\n",
    "        #Any words with \"_\" is removed completely#\n",
    "        d = re.compile(r'^\\_+$')\n",
    "        #Any words with \"*\" is removed completely#\n",
    "        e = re.compile(r'^\\*+$')\n",
    "        #Any words containing \"-\" is removed completely#\n",
    "        f = re.compile(r'^\\-+$')\n",
    "        \n",
    "        if b.search(str(words2)) != None:\n",
    "            words3.append('111')\n",
    "        elif c.search(str(words2)) != None:\n",
    "            if d.search(str(words2)) != None:\n",
    "                words3.append('')\n",
    "            elif e.search(str(words2)) != None:\n",
    "                words3.append('')\n",
    "            elif f.search(str(words2)) != None:\n",
    "                words3.append('')\n",
    "            else:\n",
    "                en_stop = get_stop_words('en')\n",
    "                if words2.lower() in en_stop:\n",
    "                    words3.append('')\n",
    "                else:\n",
    "                    p_stemmer = PorterStemmer()\n",
    "                    words3.append(p_stemmer.stem(words2.lower()))\n",
    "    return ' '.join(words3)\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "def P5():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "    count_vect1 = CountVectorizer()\n",
    "    YYY = count_vect1.fit_transform(train_data)\n",
    "    YYZ = count_vect1.transform(dev_data)\n",
    "    \n",
    "    LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = 0.2, solver='lbfgs')\n",
    "    YY = LG.fit(YYY, train_labels)\n",
    "    Z = len(count_vect1.vocabulary_)\n",
    "    print(\"This is the length of the vocabulary without pre-processing\")\n",
    "    print(Z)\n",
    "    predict = LG.predict(YYZ)\n",
    "    correct, total = 0, 0\n",
    "    for pred, label in zip(predict, dev_labels):\n",
    "        if pred == label: \n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print (\"The accuracy of the training-dev data with Logistic_Regression and C-Regularization Strength = 0.2 is \",  correct/total)\n",
    "    \n",
    "    #This is for the pre-processed word-list\n",
    "    \n",
    "    count_vect= CountVectorizer(preprocessor= better_preprocessor)\n",
    "    YYY = count_vect.fit_transform(train_data)\n",
    "    YYZ = count_vect.transform(dev_data)\n",
    "    \n",
    "    LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = 0.2, solver='lbfgs')\n",
    "    YY = LG.fit(YYY, train_labels)\n",
    "    s= len(count_vect.vocabulary_)\n",
    "    print(\"This is the length of the vocabulary post pre-processing\")\n",
    "    print(s)\n",
    "    predict = LG.predict(YYZ)\n",
    "    correct, total = 0, 0\n",
    "    for pred, label in zip(predict, dev_labels):\n",
    "        if pred == label: \n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print (\"The accuracy of the training-dev data with Logistic_Regression and C-Regularization Strength = 0.2 with custom pre-processing is\",  correct/total)\n",
    "    \n",
    "### STUDENT END ###\n",
    "P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary size was reduced from 26879 to 12612 but accuracy of the prediction reduced from 70% to 64% bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? \n",
    "\n",
    "The Logistic regression module with L1 regularization has a lot less non-Zero values than the L2 regularization model\n",
    "\n",
    "\n",
    "\n",
    "Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001).\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of non-zero features in the L1 regularization model with tolerance = 0.01 are\n",
      "2516\n",
      "The number of non-zero features in the L2 regularization model are\n",
      "26733\n",
      "    Amount_Non_Zeros  Accuracy\n",
      "0             2305.0  0.306213\n",
      "1             2190.0  0.383136\n",
      "2             2213.0  0.409763\n",
      "3             2365.0  0.483728\n",
      "4             2271.0  0.498521\n",
      "5             2379.0  0.514793\n",
      "6             2366.0  0.525148\n",
      "7             2362.0  0.532544\n",
      "8             2414.0  0.531065\n",
      "9             2469.0  0.535503\n",
      "10            2458.0  0.541420\n",
      "11            2485.0  0.550296\n",
      "12            2468.0  0.548817\n",
      "13            2492.0  0.550296\n",
      "14            2496.0  0.547337\n",
      "15            2486.0  0.547337\n",
      "16            2484.0  0.547337\n",
      "17            2492.0  0.548817\n",
      "18            2477.0  0.551775\n",
      "19            2492.0  0.554734\n",
      "20            2495.0  0.554734\n",
      "21            2485.0  0.556213\n",
      "22            2493.0  0.556213\n",
      "23            2482.0  0.554734\n",
      "24            2492.0  0.554734\n",
      "25            2501.0  0.556213\n",
      "26            2500.0  0.554734\n",
      "27            2500.0  0.557692\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a110d4860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    ### STUDENT START ###\n",
    "    \n",
    "    ###L1 Regularization Algorithm###\n",
    "    count_vect= CountVectorizer()\n",
    "    YYY = count_vect.fit_transform(train_data)\n",
    "    YYZ = count_vect.transform(dev_data)\n",
    "    \n",
    "    LG = LogisticRegression(penalty='l1',multi_class='ovr',C = 0.2, tol = 0.01, solver='saga')\n",
    "    YYTT = LG.fit(YYY, train_labels)\n",
    "\n",
    "    YYZZ = YYTT.coef_\n",
    "    T=0\n",
    "    for iii in range(YYZZ.shape[1]):\n",
    "        if sum(YYZZ[:,(iii-1)])==0:\n",
    "            T= T+1\n",
    "    print(\"The number of non-zero features in the L1 regularization model with tolerance = 0.01 are\")    \n",
    "    print((YYZZ.shape[1] -T))\n",
    "    \n",
    "    T1=[]\n",
    "    for iv in range(YYZZ.shape[1]):\n",
    "        if sum(YYZZ[:,(iv-1)])!= 0:\n",
    "            T1.append(iv)\n",
    "    \n",
    "    YES = YYY[:,T1]\n",
    "    YES3 = YYZ[:,T1]\n",
    "    ### L2 Regularization Algorithm ###\n",
    "    \n",
    "    LG2 = LogisticRegression(penalty='l2',multi_class='multinomial',C = 0.2, solver='lbfgs')\n",
    "    YYTT2 = LG2.fit(YYY, train_labels)\n",
    "\n",
    "    YYZZ2 = YYTT2.coef_\n",
    "    T2 = 0\n",
    "    for iii in range(YYZZ2.shape[1]):\n",
    "        if sum(YYZZ2[:,(iii-1)])==0:\n",
    "            T2= T2+1\n",
    "        \n",
    "    print(\"The number of non-zero features in the L2 regularization model are\")  \n",
    "    print((YYZZ2.shape[1]-T2))\n",
    "    \n",
    "    \n",
    "    ### Using the values from the L1 non-zero regularization ### \n",
    "    Non_Zero = []\n",
    "    Accuracy = []\n",
    "    Regular_Strength = [0.00001,0.0001,0.001,0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0]\n",
    "    for xx in Regular_Strength:\n",
    "        LG3 = LogisticRegression(penalty='l2',multi_class='multinomial',C = xx, solver='lbfgs')\n",
    "        YES2 = LG3.fit(YES, train_labels)\n",
    "\n",
    "        YYZZ = YES2.coef_\n",
    "        T=0\n",
    "        for iii in range(YYZZ.shape[1]):\n",
    "            if sum(YYZZ[:,(iii-1)])==0:\n",
    "                T= T+1\n",
    "                \n",
    "        TRUST= YYZZ.shape[1] -T\n",
    "        Non_Zero.append(TRUST)\n",
    "\n",
    "        \n",
    "        predict = LG3.predict(YES3)\n",
    "        correct, total = 0, 0\n",
    "        for pred, label in zip(predict, dev_labels):\n",
    "            if pred == label: \n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        Accuracy.append(correct/total)\n",
    "\n",
    "            \n",
    "    # Making a dataframe table that can be used for plotting the accuracy data vs the pruning from C-regularization #\n",
    "    True3= np.vstack((Non_Zero,Accuracy))\n",
    "    True3= True3.transpose()\n",
    "    Column_Names1= ['Amount_Non_Zeros', 'Accuracy']\n",
    "    True4 = pd.DataFrame(True3,columns= Column_Names1)\n",
    "    print(True4)\n",
    "    \n",
    "    #Making a scatter plot of the Amount_Non_Zeros vs Accuracy#\n",
    "    sns.regplot(x=True4[\"Amount_Non_Zeros\"], y=True4[\"Accuracy\"], fit_reg=False)\n",
    "\n",
    "    ### STUDENT END ###\n",
    "P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TFIDF_Vectorizer is 0.7588757396449705\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True-Values</th>\n",
       "      <th>Predicted-Values</th>\n",
       "      <th>R_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            True-Values    Predicted-Values R_Value\n",
       "655  talk.religion.misc         alt.atheism      95\n",
       "184           sci.space         alt.atheism       9\n",
       "16          alt.atheism  talk.religion.misc       9\n",
       "512         alt.atheism  talk.religion.misc       9\n",
       "561         alt.atheism  talk.religion.misc       9\n",
       "181         alt.atheism  talk.religion.misc      84\n",
       "389           sci.space  talk.religion.misc       8\n",
       "560         alt.atheism           sci.space       8\n",
       "89          alt.atheism  talk.religion.misc       8\n",
       "368  talk.religion.misc       comp.graphics       8\n",
       "607         alt.atheism  talk.religion.misc      76\n",
       "287         alt.atheism  talk.religion.misc      74\n",
       "150           sci.space         alt.atheism       7\n",
       "82   talk.religion.misc       comp.graphics       7\n",
       "60          alt.atheism  talk.religion.misc       7\n",
       "594         alt.atheism  talk.religion.misc       7\n",
       "643         alt.atheism           sci.space       7\n",
       "665  talk.religion.misc       comp.graphics     653\n",
       "373  talk.religion.misc         alt.atheism       6\n",
       "160           sci.space       comp.graphics       6\n",
       "321       comp.graphics           sci.space       6\n",
       "169           sci.space       comp.graphics       6\n",
       "52          alt.atheism  talk.religion.misc      59\n",
       "463  talk.religion.misc         alt.atheism      57\n",
       "209         alt.atheism           sci.space      56\n",
       "608           sci.space       comp.graphics       5\n",
       "154         alt.atheism           sci.space       5\n",
       "432           sci.space       comp.graphics       5\n",
       "491           sci.space       comp.graphics       5\n",
       "264  talk.religion.misc         alt.atheism       5\n",
       "..                  ...                 ...     ...\n",
       "269       comp.graphics       comp.graphics       1\n",
       "270         alt.atheism         alt.atheism       1\n",
       "271         alt.atheism         alt.atheism       1\n",
       "272           sci.space           sci.space       1\n",
       "273           sci.space           sci.space       1\n",
       "274         alt.atheism         alt.atheism       1\n",
       "275       comp.graphics       comp.graphics       1\n",
       "276       comp.graphics       comp.graphics       1\n",
       "277  talk.religion.misc  talk.religion.misc       1\n",
       "278           sci.space           sci.space       1\n",
       "261         alt.atheism         alt.atheism       1\n",
       "259       comp.graphics       comp.graphics       1\n",
       "239       comp.graphics       comp.graphics       1\n",
       "258         alt.atheism         alt.atheism       1\n",
       "240           sci.space           sci.space       1\n",
       "241           sci.space           sci.space       1\n",
       "242           sci.space           sci.space       1\n",
       "243       comp.graphics       comp.graphics       1\n",
       "244       comp.graphics       comp.graphics       1\n",
       "248           sci.space           sci.space       1\n",
       "249  talk.religion.misc  talk.religion.misc       1\n",
       "250  talk.religion.misc           sci.space       1\n",
       "251  talk.religion.misc  talk.religion.misc       1\n",
       "252           sci.space           sci.space       1\n",
       "253         alt.atheism         alt.atheism       1\n",
       "254           sci.space       comp.graphics       1\n",
       "255       comp.graphics       comp.graphics       1\n",
       "256           sci.space           sci.space       1\n",
       "257       comp.graphics           sci.space       1\n",
       "675  talk.religion.misc  talk.religion.misc       1\n",
       "\n",
       "[676 rows x 3 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def P7():\n",
    "    ### STUDENT START ###\n",
    "    tfid_vect= TfidfVectorizer()\n",
    "    YYY = tfid_vect.fit_transform(train_data)\n",
    "    YYZ = tfid_vect.transform(dev_data)\n",
    "    \n",
    "    LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = 100, solver='lbfgs')\n",
    "    YY = LG.fit(YYY, train_labels)\n",
    "    \n",
    "    ERR= YY.predict_proba(YYZ)\n",
    "    ERR2= ERR.tolist()\n",
    "    \n",
    "    predict = LG.predict(YYZ)\n",
    "    correct, total = 0, 0\n",
    "    for pred, label in zip(predict, dev_labels):\n",
    "        if pred == label: \n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print(\"Accuracy of TFIDF_Vectorizer is\", correct/total)\n",
    "    \n",
    "    \n",
    "    ###This section is to print out the Dev_Labels###\n",
    "    ii=0\n",
    "    RealValueDev = []\n",
    "    PredictedValueDev = []\n",
    "    R_Value = []\n",
    "    for i in ERR2:\n",
    "        y = max(i)\n",
    "        yy = i.index(y)\n",
    "        Valueyy = categories[yy]\n",
    "        Dev= dev_labels[ii]\n",
    "        ValueDev =  categories[Dev]\n",
    "        R= y/i[Dev]\n",
    "        ii= ii+1\n",
    "    \n",
    "    ###Appending all the values to a List for eventual computation and mistake visualization###\n",
    "        RealValueDev.append(ValueDev)\n",
    "        PredictedValueDev.append(Valueyy)\n",
    "        R_Value.append(int(R))\n",
    "\n",
    "    ###Making a dataframe- Easier to work with###\n",
    "    True3= np.vstack((RealValueDev, PredictedValueDev, (R_Value)))\n",
    "    True3= True3.transpose()\n",
    "    Column_Names1= ['True-Values', 'Predicted-Values', 'R_Value']\n",
    "    True4 = pd.DataFrame(True3,columns= Column_Names1)\n",
    "    return(True4.sort_values('R_Value', ascending = False))\n",
    "    \n",
    "    \n",
    "    ### STUDENT END ###\n",
    "P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER 1: The Test Frequency- Inverse document frequency (TF-IDF) takes the inverse of the frequency count of each feature thus features that occur rarely are given more weights than feature that occur more frequently. This is different than the countVectorizer that by default outputs all the features without any weightage on the frequency of coccurence etc.\n",
    "\n",
    "\n",
    "ANSWER 2: The model is making wrong predictions on the alt.atheism and the talk.religion.misc topics ...... These two topic sets appear to be very similar in scope and thus not surprising that the algorithm is assuming for the most part (and with high degree of confidence that one is another.\n",
    "\n",
    "A simple approach would be to combine both into a class called 'Religion.' Although we do loose granularity in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) EXTRA CREDIT\n",
    "\n",
    "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model.\n",
    "\n",
    "- [1 pt] for a reasonable attempt\n",
    "- [2 pts] for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taiwoalabi'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def better_preprocessor2(s):\n",
    "### STUDENT START ###\n",
    "\n",
    "        ###Creating a word tokenizer that accepts a group of words###\n",
    "    \n",
    "    #This can be considered my tokenizer function#\n",
    "    words3= []\n",
    "    words2= s    \n",
    "    #The digits are selected and replaced with 999#\n",
    "    b = re.compile(r'^\\d+$')\n",
    "    c = re.compile(r'^\\w+$')\n",
    "    #Any words with \"_\" is removed completely#\n",
    "    d = re.compile(r'^\\_+$')\n",
    "    #Any words with \"*\" is removed completely#\n",
    "    e = re.compile(r'^\\*+$')\n",
    "    #Any words containing \"-\" is removed completely#\n",
    "    f = re.compile(r'^\\-+$')\n",
    "    #Any words containing \"?\" is removed completely#\n",
    "    g = re.compile(r'^\\?+$') \n",
    "    #Any words containing \"@\" is removed completely#\n",
    "    h = re.compile(r'^\\@+$')\n",
    "    if b.search(str(words2)) != None:\n",
    "        words3.append('1')\n",
    "    elif c.search(str(words2)) != None:\n",
    "        if d.search(str(words2)) != None:\n",
    "            words3.append('')\n",
    "        elif e.search(str(words2)) != None:\n",
    "            words3.append('')\n",
    "        elif f.search(str(words2)) != None:\n",
    "            words3.append('')\n",
    "        elif g.search(str(words2)) != None:\n",
    "            words3.append('')\n",
    "        elif h.search(str(words2)) != None:\n",
    "            words3.append('')\n",
    "        else:\n",
    "            en_stop = get_stop_words('en')\n",
    "            if words2.lower() in en_stop:\n",
    "                words3.append('')\n",
    "            else:\n",
    "                p_stemmer = PorterStemmer()\n",
    "                words3.append(p_stemmer.stem(words2.lower()))\n",
    "    return ' '.join(words3)\n",
    "\n",
    "\n",
    "better_preprocessor2(\"TaiwoAlabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TFIDF_Vectorizer is 0.3121301775147929\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True-Values</th>\n",
       "      <th>Predicted-Values</th>\n",
       "      <th>R_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            True-Values    Predicted-Values R_Value\n",
       "62          alt.atheism           sci.space      96\n",
       "60          alt.atheism  talk.religion.misc      92\n",
       "272           sci.space         alt.atheism      91\n",
       "629           sci.space       comp.graphics       4\n",
       "565         alt.atheism       comp.graphics     283\n",
       "447  talk.religion.misc           sci.space     211\n",
       "665  talk.religion.misc           sci.space     211\n",
       "357  talk.religion.misc       comp.graphics       2\n",
       "277  talk.religion.misc           sci.space     110\n",
       "450           sci.space           sci.space       1\n",
       "446           sci.space           sci.space       1\n",
       "448         alt.atheism           sci.space       1\n",
       "449       comp.graphics           sci.space       1\n",
       "451           sci.space           sci.space       1\n",
       "452       comp.graphics           sci.space       1\n",
       "453  talk.religion.misc           sci.space       1\n",
       "454       comp.graphics           sci.space       1\n",
       "455  talk.religion.misc           sci.space       1\n",
       "456       comp.graphics           sci.space       1\n",
       "457  talk.religion.misc           sci.space       1\n",
       "458         alt.atheism           sci.space       1\n",
       "459       comp.graphics           sci.space       1\n",
       "460  talk.religion.misc           sci.space       1\n",
       "445         alt.atheism           sci.space       1\n",
       "0             sci.space           sci.space       1\n",
       "444  talk.religion.misc           sci.space       1\n",
       "461           sci.space           sci.space       1\n",
       "426       comp.graphics       comp.graphics       1\n",
       "427         alt.atheism           sci.space       1\n",
       "428           sci.space           sci.space       1\n",
       "..                  ...                 ...     ...\n",
       "244       comp.graphics           sci.space       1\n",
       "245  talk.religion.misc           sci.space       1\n",
       "246         alt.atheism           sci.space       1\n",
       "247  talk.religion.misc           sci.space       1\n",
       "248           sci.space           sci.space       1\n",
       "249  talk.religion.misc           sci.space       1\n",
       "250  talk.religion.misc           sci.space       1\n",
       "251  talk.religion.misc           sci.space       1\n",
       "234         alt.atheism           sci.space       1\n",
       "232       comp.graphics           sci.space       1\n",
       "213       comp.graphics           sci.space       1\n",
       "231           sci.space           sci.space       1\n",
       "214           sci.space           sci.space       1\n",
       "215  talk.religion.misc           sci.space       1\n",
       "216       comp.graphics           sci.space       1\n",
       "217       comp.graphics           sci.space       1\n",
       "218           sci.space           sci.space       1\n",
       "219         alt.atheism           sci.space       1\n",
       "220  talk.religion.misc           sci.space       1\n",
       "221           sci.space           sci.space       1\n",
       "222       comp.graphics           sci.space       1\n",
       "223  talk.religion.misc           sci.space       1\n",
       "224           sci.space           sci.space       1\n",
       "225         alt.atheism           sci.space       1\n",
       "226         alt.atheism           sci.space       1\n",
       "227  talk.religion.misc           sci.space       1\n",
       "228           sci.space           sci.space       1\n",
       "229           sci.space           sci.space       1\n",
       "230       comp.graphics           sci.space       1\n",
       "675  talk.religion.misc           sci.space       1\n",
       "\n",
       "[676 rows x 3 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def better_preprocessor2(s):\n",
    "### STUDENT START ###\n",
    "\n",
    "    \n",
    "    words1= s.split(' ')\n",
    "    words3= []\n",
    "    for words2 in words1:\n",
    "    #This can be considered my tokenizer function#  \n",
    "    #The digits are selected and replaced with 999#\n",
    "        b = re.compile(r'^\\d+$')\n",
    "        c = re.compile(r'^\\w+$')\n",
    "        #Any words with \"_\" is removed completely#\n",
    "        d = re.compile(r'^\\_+$')\n",
    "        #Any words with \"*\" is removed completely#\n",
    "        e = re.compile(r'^\\*+$')\n",
    "        #Any words containing \"-\" is removed completely#\n",
    "        f = re.compile(r'^\\-+$')\n",
    "        #Any words containing \"?\" is removed completely#\n",
    "        g = re.compile(r'^\\?+$') \n",
    "        #Any words containing \"@\" is removed completely#\n",
    "        h = re.compile(r'^\\@+$')\n",
    "        if b.search(str(words2)) != None:\n",
    "            words3.append('1')\n",
    "        elif c.search(str(words2)) != None:\n",
    "            #if d.search(str(words2)) != None:\n",
    "                #words3.append('')\n",
    "            #elif e.search(str(words2)) != None:\n",
    "                #words3.append('')\n",
    "            #elif f.search(str(words2)) != None:\n",
    "                #words3.append('')\n",
    "            #elif g.search(str(words2)) != None:\n",
    "                #words3.append('')\n",
    "            #elif h.search(str(words2)) != None:\n",
    "                #words3.append('')\n",
    "            #else:\n",
    "            en_stop = get_stop_words('en')\n",
    "            if words2.lower() in en_stop:\n",
    "                words3.append('')\n",
    "            else:\n",
    "                p_stemmer = PorterStemmer()\n",
    "                words3.append(p_stemmer.stem(words2.lower()))\n",
    "        return ' '.join(words3)\n",
    "\n",
    "\n",
    "\n",
    "def P7():\n",
    "    ### STUDENT START ###\n",
    "    tfid_vect= TfidfVectorizer(preprocessor =better_preprocessor2)\n",
    "    YYY = tfid_vect.fit_transform(train_data)\n",
    "    YYZ = tfid_vect.transform(dev_data)\n",
    "    \n",
    "    LG = LogisticRegression(penalty='l2',multi_class='multinomial',C = 100, solver='lbfgs')\n",
    "    YY = LG.fit(YYY, train_labels)\n",
    "    \n",
    "    ERR= YY.predict_proba(YYZ)\n",
    "    ERR2= ERR.tolist()\n",
    "    \n",
    "    predict = LG.predict(YYZ)\n",
    "    correct, total = 0, 0\n",
    "    for pred, label in zip(predict, dev_labels):\n",
    "        if pred == label: \n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print(\"Accuracy of TFIDF_Vectorizer is\", correct/total)\n",
    "    \n",
    "    \n",
    "    ###This section is to print out the Dev_Labels###\n",
    "    ii=0\n",
    "    RealValueDev = []\n",
    "    PredictedValueDev = []\n",
    "    R_Value = []\n",
    "    for i in ERR2:\n",
    "        y = max(i)\n",
    "        yy = i.index(y)\n",
    "        Valueyy = categories[yy]\n",
    "        Dev= dev_labels[ii]\n",
    "        ValueDev =  categories[Dev]\n",
    "        R= y/i[Dev]\n",
    "        ii= ii+1\n",
    "    \n",
    "    ###Appending all the values to a List for eventual computation and mistake visualization###\n",
    "        RealValueDev.append(ValueDev)\n",
    "        PredictedValueDev.append(Valueyy)\n",
    "        R_Value.append(int(R))\n",
    "\n",
    "    ###Making a dataframe- Easier to work with###\n",
    "    True3= np.vstack((RealValueDev, PredictedValueDev, (R_Value)))\n",
    "    True3= True3.transpose()\n",
    "    Column_Names1= ['True-Values', 'Predicted-Values', 'R_Value']\n",
    "    True4 = pd.DataFrame(True3,columns= Column_Names1)\n",
    "    return(True4.sort_values('R_Value', ascending = False))\n",
    "    \n",
    "    \n",
    "    ### STUDENT END ###\n",
    "P7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
